{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8870ac97",
   "metadata": {},
   "source": [
    "The previous script makes the JSON file instructions for CN for each image. \n",
    "This script feeds the JSON and the images to CN via Dropbox. \n",
    "This would be the script to amend if we wanted to use OneDrive or some other online file folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81811bd",
   "metadata": {},
   "source": [
    "First, initiate functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fee027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def get_points(height, width, h_offset, w_offset, percentage, sampling_method):\n",
    "    '''\n",
    "    height: the height of the image (rows)\n",
    "    width: the width of the image (columns)\n",
    "    offset: the % of pixels on all sides to avoid sampling (i.e. avoids edges of image)\n",
    "    percentage: the % of points to sample (1% of a 4MP image = 40,000 points)\n",
    "    sampling_method: either \"random\" or \"grid\"\n",
    "    '''\n",
    "\n",
    "    percentage = percentage * .01\n",
    "\n",
    "    num_points = int(height * width * percentage)\n",
    "\n",
    "    if(sampling_method == 'random'):\n",
    "\n",
    "        x = np.random.randint(w_offset, width - w_offset, num_points)\n",
    "        y = np.random.randint(h_offset, height - h_offset, num_points)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        density = int(np.sqrt(num_points)) \n",
    "\n",
    "        # Creates an equally spaced grid, reshapes, converts into list\n",
    "        x_, y_ = np.meshgrid(np.linspace(w_offset, width - w_offset, density), \n",
    "                             np.linspace(h_offset, height - h_offset, density))\n",
    "\n",
    "        xy = np.dstack([x_, y_]).reshape(-1, 2).astype(int)\n",
    "\n",
    "        x = [point[0] for point in xy]\n",
    "        y = [point[1] for point in xy]\n",
    "\n",
    "        # Any labels that did not fit in the grid will be sampled randomly\n",
    "        x += np.random.randint(w_offset, width - w_offset, num_points - len(xy)).tolist()\n",
    "        y += np.random.randint(h_offset, height - h_offset, num_points - len(xy)).tolist()\n",
    "\n",
    "\n",
    "    points = []\n",
    "\n",
    "    for _ in range(num_points):\n",
    "        points.append({'row': int(y[_]), 'column': int(x[_])})\n",
    "        \n",
    "    return points\n",
    "\n",
    "\n",
    "def decode_status(r_status):\n",
    "    \n",
    "    curr_status = json.loads(r_status.content) \n",
    "    message = ''\n",
    "    \n",
    "    if 'status' in curr_status['data'][0]['attributes'].keys(): \n",
    "    \n",
    "        s = curr_status['data'][0]['attributes']['successes'] \n",
    "        f = curr_status['data'][0]['attributes']['failures'] \n",
    "        t = curr_status['data'][0]['attributes']['total']\n",
    "        status = curr_status['data'][0]['attributes']['status'] \n",
    "        ids = curr_status['data'][0]['id'].split(\",\")\n",
    "        ids = ''.join(str(_) for _ in ids)\n",
    "\n",
    "        message = 'Success: ' + str(s) + ' Failures: ' + str(f) + ' Total: ' + str(t) + ' Status: ' + str(status) + ' Ids: ' + ids\n",
    "    \n",
    "    return curr_status, message\n",
    "\n",
    "\n",
    "def check_status(r):\n",
    "    \n",
    "    # Sends a request to retrieve the completed annotations, obtains status update\n",
    "    r_status = requests.get(url = 'https://coralnet.ucsd.edu' + r.headers['Location'], \n",
    "                            headers = {\"Authorization\": f\"Token {coralnet_token}\"})\n",
    "\n",
    "    # Extracts the content from the status update\n",
    "    curr_status, message = decode_status(r_status)\n",
    "        \n",
    "    return curr_status, message    \n",
    "\n",
    "def convert_to_csv3(export):\n",
    "    \n",
    "    all_preds = pd.DataFrame()\n",
    "    \n",
    "    for i in range(len(export['data'])): #This number MUST MATCH the batch size. \n",
    "\n",
    "        image_file = export['data'][i]['id'].split(\"/\")[-1].split(\"?\")[0]\n",
    "\n",
    "        for point in export['data'][i]['attributes']['points']:\n",
    "\n",
    "            per_point = dict()\n",
    "\n",
    "            per_point['image'] = image_file\n",
    "\n",
    "            per_point['X'] = point['column']\n",
    "            per_point['Y'] = point['row']\n",
    "\n",
    "            for index, classification in enumerate(point['classifications']):\n",
    "\n",
    "                per_point['score_' + str(index + 1)] = classification['score']\n",
    "                per_point['label_id_' + str(index + 1)] = classification['label_id']\n",
    "                per_point['label_code_' + str(index + 1)] = classification['label_code']\n",
    "                per_point['label_name_' + str(index + 1)] = classification['label_name']\n",
    "\n",
    "            all_preds = pd.concat([all_preds, pd.DataFrame.from_dict([per_point])])\n",
    "    \n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be962eb3",
   "metadata": {},
   "source": [
    "Then run the images through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924c8f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change this to the local location of your json file that you made in your last script. \n",
    "#I like to keep mine in a desktop folder.\n",
    "#If you made multiple you can iterate or run file by file. \n",
    "#I actually do the latter because internet bandwidth necessitates babysitting.\n",
    "\n",
    "with open('C:\\\\LOCALLOCATIONOFMYJSONFILE.json', 'r') as file: \n",
    "  data3 = json.load(file)\n",
    "\n",
    "# How long to wait before asking for another status update\n",
    "patience = 60\n",
    "classifier_url = 'https://coralnet.ucsd.edu/api/classifier/CURRENTROBOTNUMBER/deploy/' #Change this to the current robot\n",
    "import dropbox\n",
    "import json\n",
    "import random\n",
    "import time \n",
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "#Give it a glob file of all your images\n",
    "#change this to have the correct paths.\n",
    "img_files = glob.glob('C:\\\\Users\\\\YOURUSERNAME\\\\Dropbox\\\\Apps\\\\YOURDROPBOXAPPNAME\\\\YOURDBAPPSUBFOLDERNAME\\\\*.jpg') \n",
    "\n",
    "#Defines variables\n",
    "#change this to your app subfolder name\n",
    "folder_to_use = '/APPSUBFOLDERNAME' \n",
    "\n",
    "######\n",
    "\n",
    "#This variable is the folder you want to search for on Dropbox and will also be how the resulting JSON will be named\n",
    "#change to the proper folder you like your output stored in.\n",
    "output_folder = 'C:\\\\Users\\\\YOURUSERNAME\\\\Desktop\\\\THEFOLDERYOUKEEPYOUROUTPUTIN\\\\' \n",
    "\n",
    "#Saves the classifier URL you will be using and your CoralNet authorization token\n",
    "headers = {\"Authorization\": f\"Token {coralnet_token}\", #you called this token in the last script.\n",
    "           \"Content-type\": \"application/vnd.api+json\"}\n",
    "# Looping through each file\n",
    "\n",
    "####Initiating variables for the loop.\n",
    "k=0\n",
    "dat_length=100\n",
    "\n",
    "while dat_length==100:\n",
    "    dat=data3['data'][k:k+100]\n",
    "    dat_length=len(dat)\n",
    "    if dat_length==0:\n",
    "        break\n",
    "    # For feedback purposes\n",
    "    reported = False\n",
    "    \n",
    "    # Break the data into batches of N (CoralNet requirement). 100 is the greatest number available for CN.\n",
    "    current_batch = {\"data\" : data3['data'][k : k + 100]} \n",
    "    \n",
    "    # Creates an individual request from our JSON request file\n",
    "    with open(f'{output_folder}Batch_{str(k+100)}.json', 'w') as outfile:\n",
    "        json.dump(current_batch, outfile)\n",
    "    \n",
    "    print(\"\\nCurrently on batch:\", str(k + 100), \" containing:\", len(current_batch['data']), \" entries.\")\n",
    "\n",
    "    # Sends the requests to the `source` and in exchange we recieve a message telling us if it was recieved correctly.\n",
    "    r = requests.post(url = classifier_url, data = open(f\"{output_folder}Batch_{str(k+100)}.json\"), headers = headers) \n",
    "    \n",
    "    # If request didn't go through, end the loop and change the settings according to the error\n",
    "    if(r.content.decode() != ''):\n",
    "        print(\"Error: \", r, r.content)\n",
    "        break\n",
    "    else:\n",
    "        print(\"Request sent successfully! Please wait\", patience, \"seconds\")\n",
    "\n",
    "    # Waits N seconds before attempting to retrieve results\n",
    "    time.sleep(patience)     \n",
    "    in_progress = True\n",
    "    \n",
    "    # Pings CoralNet every N seconds to check the status of the job\n",
    "    while in_progress:\n",
    "        \n",
    "        # Get an update on our request\n",
    "        curr_status, message = check_status(r)\n",
    "        \n",
    "        # Not complete yet, wait N seconds and then ask again\n",
    "        if message != '': \n",
    "            if(reported):\n",
    "                print('.')\n",
    "            else:\n",
    "                print(message)\n",
    "            reported = True\n",
    "            time.sleep(patience) \n",
    "        \n",
    "        # It's complete! Store the annotations in export, close while loop, goes to the next image.\n",
    "        else: \n",
    "            print(\"Finished \", str(k + 100), \" batch\" )\n",
    "            \n",
    "            predictions = convert_to_csv3(curr_status) #changed to my script, convert_to_csv3, to handle multiple entries\n",
    "            predictions.to_csv(f'{output_folder}Batch_{str(k+100)}.csv')\n",
    "            \n",
    "            #export['data'].extend(curr_status['data'])\n",
    "            \n",
    "            in_progress = False\n",
    "    k+=100 #reassigning _ for the next loop "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83696adf",
   "metadata": {},
   "source": [
    "Give it plenty of time to run. It should print ellipses while running.\n",
    "What to do if it stops?\n",
    "First export the curr_status and edit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785c8078",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment this if you want to print the partial outputs of a failed or stalled run.\n",
    "#change the folder paths in caps. \n",
    "\n",
    "#predfail=convert_to_csv3(curr_status)\n",
    "#predfail.to_csv('C:\\\\Users\\\\Desktop\\\\LOCALFILEPATHNAMEDWHATEVERYOULIKE.csv')\n",
    "\n",
    "#with open('C:\\\\Users\\\\YOURUSERNAME\\\\Desktop\\\\THEPLACEYOUKEEPYOURJSONSCRIPT.json', 'r') as file:\n",
    "#  data1 = json.load(file)\n",
    "#predfail=convert_to_csv3(data1)\n",
    "#predfail.to_csv('C:\\\\YOURCHOSENPATHANDNAME.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
